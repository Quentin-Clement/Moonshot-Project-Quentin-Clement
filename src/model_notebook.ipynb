{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Have a look to the Google Drive where the dataset is stored as well as previous versions of the notebook to understand the reasoning I have been through:\n",
        "https://drive.google.com/drive/folders/1jBOFqkPyN-_daZR1IP7_eBGHi-bKBHns?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PawT6rqt9aam"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "\n",
        "# method to check if the dependencies needs to be installed\n",
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "if RunningInCOLAB:\n",
        "    !pip install tensorflow opencv-python matplotlib mediapipe\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"Not running in Google Collab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-2XaC0759aal"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import random\n",
        "import concurrent.futures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9cgRhkL61UJD"
      },
      "outputs": [],
      "source": [
        "#! Quentin\n",
        "\n",
        "GOOGLE_PATH = \"./drive/MyDrive/moonshot/squat/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lg68eLfO9aam"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "#! Made constants for WIDTH and HEIGHT\n",
        "\n",
        "# Define the dimensions of the frames\n",
        "WIDTH = 240\n",
        "HEIGHT = 320\n",
        "NUM_CLASSES = 4\n",
        "\n",
        "# Define paths and sequences length\n",
        "DATA_PATH = GOOGLE_PATH + '/dataset/quentin/3/frames'\n",
        "actions = ['correct', 'depth', 'heels lifting', 'knee cave']\n",
        "SEQUENCE_LENGTH = 150  # Number of frames to consider in each sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "whmrCIM29aan"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "def resize_image(image, target_width=WIDTH, target_height=HEIGHT):\n",
        "    \"\"\"\n",
        "    Resize an image to a target width and height while keeping the original aspect ratio.\n",
        "    If the original aspect ratio is different from the target aspect ratio, the resized image is padded with zeros.\n",
        "    @param image: The image to resize\n",
        "    @param target_width: The target width set to 240 by default\n",
        "    @param target_height: The target height set to 320 by default\n",
        "    @return: The resized image\n",
        "    \"\"\"\n",
        "    original_height, original_width = image.shape[:2]\n",
        "    original_aspect_ratio = original_width / original_height\n",
        "    target_aspect_ratio = target_width / target_height\n",
        "\n",
        "    if original_aspect_ratio == target_aspect_ratio:\n",
        "        resized_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "    elif original_aspect_ratio < target_aspect_ratio:\n",
        "        scale_factor = target_height / original_height\n",
        "        new_width = int(original_width * scale_factor)\n",
        "        resized_image = cv2.resize(image, (new_width, target_height), interpolation=cv2.INTER_AREA)\n",
        "        pad_width = (target_width - new_width) // 2\n",
        "        resized_image = cv2.copyMakeBorder(resized_image, 0, 0, pad_width, pad_width, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    else:\n",
        "        scale_factor = target_width / original_width\n",
        "        new_height = int(original_height * scale_factor)\n",
        "        resized_image = cv2.resize(image, (target_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "        crop_height = (new_height - target_height) // 2\n",
        "        resized_image = resized_image[crop_height:crop_height + target_height, :]\n",
        "\n",
        "    return resized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_ZofwpBWiZHf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_pose_keypoints(results):\n",
        "    \"\"\"\n",
        "    Extract pose keypoints from MediaPipe results.\n",
        "    @param results: MediaPipe results\n",
        "    @return: Flattened array of pose keypoints\n",
        "    \"\"\"\n",
        "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
        "    return pose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nSWiuWZplvQI"
      },
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Pose\n",
        "mp_pose = mp.solutions.pose\n",
        "\n",
        "def extract_and_save_keypoints_as_numpy(video_path, output_dir, target_fps=30, max_frames=150):\n",
        "    \"\"\"\n",
        "    Extract pose keypoints from a video file and save them as numpy arrays in the specified output directory.\n",
        "    @param video_path: The path to the video file\n",
        "    @param output_dir: The directory where the extracted keypoints will be saved\n",
        "    @param target_fps: The frame rate at which to extract the frames\n",
        "    @param max_frames: The maximum number of frames to extract\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_interval = int(original_fps / target_fps)\n",
        "    frame_count = 0\n",
        "    saved_frame_count = 0\n",
        "    last_keypoints = None\n",
        "\n",
        "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_interval == 0:\n",
        "                # Convert the BGR image to RGB\n",
        "                image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                image.flags.writeable = False\n",
        "\n",
        "                # Process the image and extract the pose landmarks\n",
        "                results = pose.process(image)\n",
        "\n",
        "                # Extract keypoints\n",
        "                keypoints = extract_pose_keypoints(results)\n",
        "                frame_filename = os.path.join(output_dir, f'frame_{saved_frame_count:04d}.npy')\n",
        "                np.save(frame_filename, keypoints)\n",
        "                saved_frame_count += 1\n",
        "                last_keypoints = keypoints\n",
        "\n",
        "                if saved_frame_count >= max_frames:\n",
        "                    break\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "    # Duplicate the last frame if there are less than 150 frames originally\n",
        "    while saved_frame_count < max_frames and last_keypoints is not None:\n",
        "        frame_filename = os.path.join(output_dir, f'frame_{saved_frame_count:04d}.npy')\n",
        "        np.save(frame_filename, last_keypoints)\n",
        "        saved_frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(f\"Extracted and saved {saved_frame_count} frames for {video_path} as numpy arrays.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "h7xODAlxyEHK"
      },
      "outputs": [],
      "source": [
        "# Define the main paths\n",
        "main_path = '/content/drive/My Drive/moonshot/squat/dataset/quentin/3'\n",
        "frame_path = os.path.join(main_path, 'frames')\n",
        "categories = ['correct', 'depth', 'heels lifting', 'knee cave']\n",
        "\n",
        "# Create the frames directory if it doesn't exist\n",
        "os.makedirs(frame_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrIHITMryGQS"
      },
      "outputs": [],
      "source": [
        "# Verify if the main path exists and list its contents\n",
        "if os.path.exists(main_path):\n",
        "    print(f\"The main path exists: {main_path}\")\n",
        "    print(\"Listing contents of the main path:\")\n",
        "    print(os.listdir(main_path))\n",
        "else:\n",
        "    print(f\"The main path does not exist: {main_path}\")\n",
        "\n",
        "# Loop through each category and process the videos\n",
        "for category in categories:\n",
        "    category_path = os.path.join(main_path, category)\n",
        "    if os.path.exists(category_path):\n",
        "        print(f\"Processing category: {category}\")\n",
        "        videos = sorted(os.listdir(category_path))\n",
        "\n",
        "        for video in videos:\n",
        "            video_name = os.path.splitext(video)[0]\n",
        "            video_path = os.path.join(category_path, video)\n",
        "            output_dir = os.path.join(frame_path, category, video_name)\n",
        "            extract_and_save_keypoints_as_numpy(video_path, output_dir)\n",
        "    else:\n",
        "        print(f\"Category path does not exist: {category_path}\")\n",
        "\n",
        "print(\"Frame extraction and resizing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWLs56SGxiOW"
      },
      "outputs": [],
      "source": [
        "def print_numpy_shapes(main_path, categories):\n",
        "    \"\"\"\n",
        "    Print the shape of each numpy array saved in the specified directory and its subdirectories.\n",
        "    @param base_path: The base directory containing category directories\n",
        "    @param categories: List of categories to process\n",
        "    \"\"\"\n",
        "    for category in categories:\n",
        "        category_path = os.path.join(main_path, 'frames', category)\n",
        "        for video_folder in os.listdir(category_path):\n",
        "            video_folder_path = os.path.join(category_path, video_folder)\n",
        "            if os.path.isdir(video_folder_path):  # Ensure it's a directory\n",
        "                for frame_file in os.listdir(video_folder_path):\n",
        "                    frame_path = os.path.join(video_folder_path, frame_file)\n",
        "                    if frame_path.endswith('.npy'):\n",
        "                        frame = np.load(frame_path)\n",
        "                        print(f\"Shape of {frame_path}: {frame.ndim}\")\n",
        "\n",
        "# Print the shape of each numpy array\n",
        "print_numpy_shapes(main_path, categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BPp51TA6WjtC"
      },
      "outputs": [],
      "source": [
        "# Function to load a single frame from a numpy file\n",
        "def load_frame(frame_path):\n",
        "  \"\"\"\n",
        "  Load a single frame from a numpy file.\n",
        "  @param frame_path: The path to the numpy file containing the frame\n",
        "  @return: The loaded frame\n",
        "  \"\"\"\n",
        "  if os.path.exists(frame_path):\n",
        "    return np.load(frame_path)\n",
        "  else:\n",
        "      print(f\"Frame {frame_path} does not exist, padding with zeros\")\n",
        "      return np.zeros((WIDTH, HEIGHT, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HDyVj8juXUqR"
      },
      "outputs": [],
      "source": [
        "# Function to load frames for a single sequence\n",
        "def load_sequence(sequence_path, sequence_length):\n",
        "    \"\"\"\n",
        "    Load frames for a single sequence.\n",
        "    @param sequence_path: The path to the directory containing the frames for the sequence\n",
        "    @param sequence_length: The number of frames in the sequence\n",
        "    @return: The loaded frames\n",
        "    \"\"\"\n",
        "    frame_paths = [os.path.join(sequence_path, f\"frame_{i:04d}.npy\") for i in range(sequence_length)]\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        frames = list(executor.map(load_frame, frame_paths))\n",
        "    return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKY8L4kG9aan"
      },
      "outputs": [],
      "source": [
        "# Create a label map\n",
        "label_map = {label:num for num, label in enumerate(actions)}\n",
        "print(label_map)\n",
        "\n",
        "# Initialize sequences and labels as empty lists\n",
        "sequences, labels = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4yYqqLP9aao"
      },
      "outputs": [],
      "source": [
        "# Loop through each action\n",
        "for action in actions:\n",
        "    action_path = os.path.join(DATA_PATH, action)\n",
        "    if os.path.exists(action_path):\n",
        "        for sequence in sorted(os.listdir(action_path)):\n",
        "            sequence_path = os.path.join(action_path, sequence)\n",
        "            window = load_sequence(sequence_path, SEQUENCE_LENGTH)\n",
        "            sequences.append(window)\n",
        "            labels.append(label_map[action])\n",
        "    else:\n",
        "        print(f\"Action path {action_path} does not exist\")\n",
        "\n",
        "# Convert sequences and labels to numpy arrays\n",
        "sequences = np.array(sequences)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(sequences.shape)\n",
        "print(labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYnF3ZdI9aao"
      },
      "outputs": [],
      "source": [
        "# Prepare the data for training\n",
        "X = sequences\n",
        "y = to_categorical(labels).astype(int)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2dlOgNoK-mM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, LayerNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True, activation='relu'), input_shape=(SEQUENCE_LENGTH, 132)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Additional Bidirectional LSTM Layer\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True, activation='relu')))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Another LSTM Layer\n",
        "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense Layers\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LayerNormalization())\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LayerNormalization())\n",
        "\n",
        "# Output Layer\n",
        "model.add(Dense(len(actions), activation='softmax'))\n",
        "\n",
        "# Compile the model with gradient clipping\n",
        "optimizer = Adam(learning_rate=0.0005, clipvalue=1.0)  # Reduced learning rate and gradient clipping\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Setup TensorBoard and EarlyStopping\n",
        "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "X_train_processed = X_train.reshape(X_train.shape[0], SEQUENCE_LENGTH, 132)\n",
        "X_test_processed = X_test.reshape(X_test.shape[0], SEQUENCE_LENGTH, 132)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_processed, y_train, epochs=100, validation_data=(X_test_processed, y_test), callbacks=[tensorboard, early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAQBBU85pyjS"
      },
      "outputs": [],
      "source": [
        "# Predictions\n",
        "res = model.predict(X_test)\n",
        "\n",
        "for i in range(len(y_test)):\n",
        "    pred = actions[np.argmax(res[i])]\n",
        "    truth = actions[np.argmax(y_test[i])]\n",
        "    print(pred, truth)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
